
\section{Introduction}

This paper details the cryptographic techniques and protocols used by Cape Privacy's encrypted learning solution. We expect the reader to have some familiarity with secure multi-party computation, linear algebra, and ring arithmetic; as a good starting point we recommend \cite{evans2017pragmatic}. We also omit detailed explanations of machine learning and the models we support, instead referring the interested reader to~\cite{??}.

Note that this paper is currently scoped to the linear regression task outlined in section~\ref{??}. Future versions of the paper will expand upon this with additional models and protocols.


\subsection{Linear Regression}

We here focus on performing linear regression between a data provider inputting $y_{\mathit{true}}$ and a data subscriber inputting $x$. By appropriately processing the data in plaintext on the data subscriber side before it is secret shared, we can derive a $x'$ so that the operations needed to subsequently be performed on secret shared data boils down to a single matrix dot product:
\begin{align*}
w = x' \cdot y_{\mathit{true}}
\end{align*}

In addition to this, we also need operations for computing several metrics on the secret shared data to measure the impact of the regression which means. Concretely, we present cryptographic protocols to support mean squared error (MSE) and residual sum of squares (RSS).

$$
w = x' \cdot y_{\mathit{true}}
$$

$$
y_{\mathit{pred}} = x \cdot w
$$

\begin{align*}
\mathit{MSE}(y_{\mathit{pred}}, y_{\mathit{true}}) = \sum \frac{ (y_{\mathit{pred}} - y_{\mathit{true}})^2 }{n}
\\
\mathit{RSS}(y_{\mathit{pred}}, y_{\mathit{true}}) = \sum \left( (y_{\mathit{pred}} - y_{\mathit{true}})^2 \right)
\end{align*}

In order to compute the MAPE (mean absolute percentage error) metric which boils down to compute
the absolute value of a secret $\share{|\vx|}$.

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,
%     breaklines=true,
%     captionpos=b,
%     keepspaces=true,
%     numbers=left,
%     numbersep=5pt,
%     showspaces=false,
%     tabsize=2
% }

% \lstset{style=mystyle}

% \begin{lstlisting}[language=Python]
% @edsl.computation
% def lin_reg():

%     with x_owner:
%         X = edsl.load("X")
%         bias = edsl.ones(edsl.slice(edsl.shape(X), begin=0, end=1))
%         reshaped_bias = edsl.expand_dims(bias, 1)
%         X_b = edsl.concatenate([reshaped_bias, X], axis=1)
%         A = edsl.inverse(edsl.dot(edsl.transpose(X_b), X_b))
%         B = edsl.dot(A, edsl.transpose(X_b))

%     with y_owner:
%         y_true = edsl.load("y")
%         totals_ss = ss_tot(y_true)

%     with replicated:
%         w = edsl.dot(B, y_true)
%         y_pred = edsl.dot(X_b, w)
%         mse_result = mse(y_pred, y_true)
%         residuals_ss = ss_res(y_pred, y_true)

%     with x_owner:
%         rsquared_result = r_squared(residuals_ss, totals_ss)
%         w = edsl.identity(w)
%         mse = edsl.identity(mse)
%         residuals_ss = edsl.identity(residuals_ss)
% \end{lstlisting}

\commentM{TODO: briefly describe the usecase and its high-level computation. }

\commentM{TODO: insert eDSL snippet}

For the linear regression use-case we believe that $128$ bit field size is
enough using a fixed point precision by $16$ bits while keeping the numbers
magnitude to $40$ bits (thus making the integral part of size $24$ bits).

\input{replicated/fixed-point}
