
\subsection{Linear regression}
\label{sec:linreg}

We focus on performing linear regression between two distinct data owners, one providing the explanatory variable(s) $X$ and another providing some target (or response) variable $y$. Solving the classical linear regression problem equates to finding the minimizer of the mean squared error, 
\begin{align*}
\optCoeff = \min_{\coeff}\instanceAvg (X \cdot w - y)^2
\end{align*}
Using the method of maximum likelihood estimation, we can reformulate this optimization problem into the product
\begin{align*}
\optCoeff &= (X^TX)^{-1}X^Ty
\\
&= Z \cdot y
\end{align*}
Note that evaluating $Z = (X^TX)^{-1}X^T$ can be performed as local preprocessing on plaintext data by the provider of $X$ (since the righthand term does not involve the target variable $y$). Thus, solving this regression problem securely reduces to a single secret-shared matrix-vector product $\coeff = Z \cdot y$.

In addition to solving the regression problem, we also need to compute metrics to assess the outcome of the regression on secret shared data. Concretely, we present cryptographic protocols to support mean squared error ($MSE$) and R-squared ($R^2$) metrics. Denoting the target predictions $\ypred = X \cdot \coeff$ and the mean target value as $\bar{y} = \instanceAvg y_i$, an argument similar to the above reduces the secure computation of $R^2$ to securely computing the residual sum of squares ($RSS$):
\begin{align*}
R^2 &= 1 - \frac{\sum(y_i - \ypred_i)^2}{\sum(y_i - \bar{y})^2}
\\
&= 1 - \frac{RSS(\ypred, y)}{SS(\ypred, y)}
\end{align*}
since only the $RSS$ term requires data from both providers (i.e. both $X$ and $y$). 

As a result, it suffices to present protocols for securely evaluating the following values:

\begin{align*}
\coeff &= Z \cdot y
\\
\ypred &= X \cdot \coeff
\\
\mathit{MSE}(\ypred, y) &= \instanceAvg (\ypred - y_i)^2 
\\
\mathit{RSS}(\ypred, y) &= \sum_n (\ypred - y_i)^2 
\end{align*}

%In order to compute the MAPE (mean absolute percentage error) metric which boils down to compute the absolute value of a secret $\share{|\vx|}$.

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,
%     breaklines=true,
%     captionpos=b,
%     keepspaces=true,
%     numbers=left,
%     numbersep=5pt,
%     showspaces=false,
%     tabsize=2
% }

% \lstset{style=mystyle}

% \begin{lstlisting}[language=Python]
% @edsl.computation
% def lin_reg():

%     with x_owner:
%         X = edsl.load("X")
%         bias = edsl.ones(edsl.slice(edsl.shape(X), begin=0, end=1))
%         reshaped_bias = edsl.expand_dims(bias, 1)
%         X_b = edsl.concatenate([reshaped_bias, X], axis=1)
%         A = edsl.inverse(edsl.dot(edsl.transpose(X_b), X_b))
%         B = edsl.dot(A, edsl.transpose(X_b))

%     with y_owner:
%         y_true = edsl.load("y")
%         totals_ss = ss_tot(y_true)

%     with replicated:
%         w = edsl.dot(B, y_true)
%         y_pred = edsl.dot(X_b, w)
%         mse_result = mse(y_pred, y_true)
%         residuals_ss = ss_res(y_pred, y_true)

%     with x_owner:
%         rsquared_result = r_squared(residuals_ss, totals_ss)
%         w = edsl.identity(w)
%         mse = edsl.identity(mse)
%         residuals_ss = edsl.identity(residuals_ss)
% \end{lstlisting}

Finally, all of these operations must be implemented for fixed-point numbers as described in Section~\ref{subsec:fixed-point}.
